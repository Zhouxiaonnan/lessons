# Table of contents

* [大模型源码和原理解析-Qwen2](README.md)

## 代码

* [Tokenization\_qwen2](dai-ma/tokenization\_qwen2.md)
* [Modeling\_qwen2](dai-ma/modeling\_qwen2.md)
* [Configuration\_qwen2](dai-ma/configuration\_qwen2.md)

## 原理

* [1.1 语料标准化](yuan-li/1.1-yu-liao-biao-zhun-hua.md)
* [1.2 pre tokenization](yuan-li/1.2-pre-tokenization.md)
* [1.3.1 BPE](yuan-li/1.3.1-bpe.md)
* [1.3.2 BBPE](yuan-li/1.3.2-bbpe.md)
* [2.1 word embedding](yuan-li/2.1-word-embedding.md)
* [2.2.1 绝对位置编码](yuan-li/2.2.1-jue-dui-wei-zhi-bian-ma.md)
* [2.2.2 相对位置编码](yuan-li/2.2.2-xiang-dui-wei-zhi-bian-ma.md)
* [2.2.3 RoPE](yuan-li/2.2.3-rope.md)
* [3.1.1 Normalization](yuan-li/3.1.1-normalization.md)
* [3.1.2 RMSNorm](yuan-li/3.1.2-rmsnorm.md)
* [3.3 从Post-Norm到Pre-Norm](yuan-li/3.3-cong-postnorm-dao-prenorm.md)
* [4.1 Transformers](yuan-li/4.1-transformers.md)
* [4.2.1 Scaled Dot-Product Attention](yuan-li/4.2.1-scaled-dot-product-attention.md)
* [4.2.2 multi-head self-attention](yuan-li/4.2.2-multi-head-self-attention.md)
* [4.3 attention-mask](yuan-li/4.3-attention-mask.md)
* [4.4 flash attention](yuan-li/4.4-flash-attention.md)
* [4.5 GQA](yuan-li/4.5-gqa.md)
* [5.1 Activation](yuan-li/5.1-activation.md)
* [5.2 Loss](yuan-li/5.2-loss.md)
