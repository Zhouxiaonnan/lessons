# 4.2.2 multi-head self-attention

#### multi-head self-attention（多头自注意力机制）

<figure><img src="https://pic4.zhimg.com/80/v2-d818faae12ac0af983615485e727052b_720w.webp" alt="" height="397" width="316"><figcaption></figcaption></figure>

自注意力允许序列中的每个位置都能关注序列中的所有位置，从而捕捉全局依赖关系。多头自注意力通过将Q、K和V线性投影多次到不同的空间中，然后在这些投影后的版本上并行执行缩放点积注意力，这样做可以使模型能够同时从不同的表示子空间中学习并整合信息。

就像图中演示的一样，在经过多次注意力后，将每次的输出结果进行拼接后，再经过一个线性层转换成hidden\_states的维度即可。

<figure><img src="https://pic3.zhimg.com/80/v2-417201bb658198b01632d24c1d420f7a_720w.webp" alt="" height="85" width="590"><figcaption></figcaption></figure>

论文中也提到，在他们的实验中，一共使用了h=8个头，而每个头的输入维度是hidden\_states/h，由于每个头的维度都是原始维度的1/h，所以并没有因为多头机制而导致计算效率下降。

#### Attention 可视化

<figure><img src="https://pic3.zhimg.com/80/v2-6c8b32cc54335168b80c0ee136f17ade_720w.webp" alt="" height="553" width="924"><figcaption></figcaption></figure>

<figure><img src="https://pic3.zhimg.com/80/v2-5f71c2fe0675c70c88e7bd57b92d599a_720w.webp" alt="" height="737" width="691"><figcaption></figcaption></figure>
