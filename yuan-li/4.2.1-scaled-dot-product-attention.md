# 4.2.1 Scaled Dot-Product Attention

#### Attention

Attention是这篇论文中最为主要的贡献了，要了解多头注意力机制，我们要先从最简单的注意力机制开始说起

Scaled Dot Product Attention（缩放点积注意力机制）

<figure><img src="https://pic4.zhimg.com/80/v2-b92bc0560362c5800411c52014206f87_720w.webp" alt="" height="340" width="301"><figcaption></figcaption></figure>

<figure><img src="https://pic1.zhimg.com/80/v2-92fce52d8d048a4df97eba941470ecdc_720w.webp" alt="" height="235" width="983"><figcaption></figcaption></figure>

QKV到底是什么？

可能会有同学要问了，公式中的Q，K和V到底是什么？明明输入是hidden\_states，这QKV到底是从哪里来的？

其实这里的QKV都是hidden\_states经过线性变换得到的：

𝑄=𝑋𝑊𝑄𝐾=𝑋𝑊𝐾𝑉=𝑋𝑊𝑉以上X就是hidden\_states，三个W矩阵，是可学习的参数。

那么为什么要这样命名呢？其实QKV这样命名是为了能够更好地理解attention的机制，现在就来详细说明一下我个人的理解：

* Query查询（Q）：类似于sql中的查询语句，这个矩阵就是告诉模型我们要在数据库中找到什么东西。
* Key键（K）：类似于sql表中的键，通过键我们能够找到表中的一行行数据，那么要哪些键呢？Query一下咯，所以Q和K算点积，实际上就是在做这一步。
* Value值（V）：类似于sql表中的数据，现在我们通过Query和Key的交互，得到了我们想要的数据的索引，接着我们只要取出想要的数据就好啦，怎么做呢？就是再与V算一次点击，就能取到了。

缩放是什么意思？

又有同学要问了，为什么Q和K算点积后要进行一个 1/𝑑𝑘 的缩放呢？

因为如果 𝑑𝑘 (维度数)很大的话，那么Q和K点积后结果会变得非常大，导致在后续的softmax函数后得到的梯度变得非常小，进而导致梯度消失，所以要通过缩放来消除这个问题。

mask是什么意思？

这个一会attention mask的时候讲。

为什么要经过softmax？

经过缩放后，我们可以认为得到一个分数矩阵，这个矩阵代表了序列中的每个元素是否应该关注，以及应该关注多少序列中的每个元素，经过softmax后，就转换成了权重矩阵，可以突出其关注度的相对性，也有更好的可解释性。
