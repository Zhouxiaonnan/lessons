# 1.2 pre tokenization

相比于tokenizers来说，pre\_tokenizers是相对而言更加简单更加容易理解的，预分词的作用，就是根据一组规则对输入的文本进行分割，这种预处理是为了确保模型不会在多个“分割”之间构建tokens。

比如如果不进行预分词，而是直接进行分词，那么可能出现这种情况："您好 人没了" -> "您" "好 人" "没了"。

也就是说分词有可能会产生这种与我们日常经验相悖的分词效果，而预分词就可以有效地避免这一点，比如在分词前，先在使用预分词在空格上进行分割："您好 人没了" -> "您好" "人没了"，再进行分词："您好" "人没了" -> "您好" "人" "没了"。

在tokenizers包中可以直接调用tokenizers.pre\_tokenizers中的预分词器，如果想要调用多个，也可以使用tokenizers.pre\_tokenizers.Sequence。

那么具体包含哪些预分词器呢？

<figure><img src="https://pic4.zhimg.com/80/v2-e63c64ee35303e9800472843277a8e5f_720w.webp" alt="" height="928" width="731"><figcaption></figcaption></figure>

以上图片就是tokenizers这个包里包含的所有预分词器了，理解起来都很简单，Split的话有一些参数，我们做一些尝试看看都是什么作用：

<figure><img src="https://pic1.zhimg.com/80/v2-c1a50120514d77eafa6593b844642624_720w.webp" alt="" height="197" width="728"><figcaption></figcaption></figure>

behavior：

<figure><img src="https://pic2.zhimg.com/80/v2-9487e53795dbe1d796e815dee317ed11_720w.webp" alt="" height="313" width="1398"><figcaption></figcaption></figure>

* removed：切分字符不作为一个token
* isolated：切分字符单独作为一个token
* merged\_with\_previous：切分字符与前一个单词作为一个token
* merged\_with\_next：切分字符与后一个单词作为一个token
* contiguous：多个切分字符连续，则连在一起作为一个token

这里contiguous重新举一个例子：在"Hello,"后面再加了几个空格，即可看出区别，contiguous的切分中的第二个token就是连续的空格

<figure><img src="https://pic3.zhimg.com/80/v2-55457e0d813bd756a33c8f710f5c881a_720w.webp" alt="" height="310" width="1903"><figcaption></figcaption></figure>

inverted主要是作用于给的第一个参数pattern，因为这个参数可以是字符或者是正则表达式，那么根据需要可以将正则表达式进行翻转，也就是之前是按照什么进行拆分，那么现在就按照除了pattern之外的进行拆分。

但是！这是我对这个参数的理解，可是实际情况跟我想的有些差别，在网上似乎也没有找到与实际情况相符的对这个参数的说明。

以下是inverted=True时的情况：

<figure><img src="https://pic4.zhimg.com/80/v2-09613a95b3e640d70d78bcb9dfb6ed3b_720w.webp" alt="" height="317" width="1399"><figcaption></figcaption></figure>

可见对于'removed'模式下，只保留了分隔符。而'isolated'模式下没有变化。'merged\_with\_previous'和'merged\_with\_next'互相调换。

但一般而言，只有极少情况下才会用到inverted=True，因此我也就没有再去深究了。
