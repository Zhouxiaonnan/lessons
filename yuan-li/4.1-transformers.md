# 4.1 Transformers

论文：[1706.03762 (arxiv.org)](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.03762)

自从重新出发学习LLM以来，还没有认真去看过《Attention is all you need》，那么这篇文章就是我在阅读这篇论文时的笔记总结。

首先，让我们简单看看Transformer的优势和缺陷。

### Transformer解决的问题

#### 序列模型的效率低下

传统的序列转换模型，比如基于循环神经网络（RNN）的模型，在处理较长序列时，它的效率极其低下，并且难以并行化，这就导致我们必须一个token一个token地进行训练，模型学习效率很低。

而Transformer通过Attention机制使得序列数据也可以进行并行化训练，这样即可大大提高训练效率。

下表中，显示了self-attention和recurrent的计算时间复杂度。

* complexity per layer：每一层的时间复杂度
* Sequential Operations：顺序操作时间复杂度，因为self-attention可以进行并行计算，因此是O(1)，而recurrent因为是序列顺序计算，所以是O(n)
* Maximum Path Length：最大路径长度，指的是序列中最大的依赖长度，self-attention可以直接在一个步骤内序列从头到尾的依赖，因此是O(1)，recurrent也是由于顺序计算，所以是O(n)

<figure><img src="https://pic2.zhimg.com/80/v2-511766d6c42e7351911d4a2ef0f0af3d_720w.webp" alt="" height="241" width="756"><figcaption></figcaption></figure>

#### 长距离依赖问题

在RNN中，随着一步一步线性训练的过程中，模型在较早学习到的知识会在传播过程中发生遗忘，因此无法依赖在序列中相隔较远的知识。

Transformer通过self-attention机制让模型能够捕捉序列中的长距离依赖。

#### 无法在不同子空间中捕获依赖关系

Transformer通过引入多头注意力机制（Multi-Head Attention），让它能够从不同的表示子空间中捕获信息和tokens之间的依赖关系。

#### 复杂的模型结构

相比于RNN（LSTM，GRU），Transformer的模型结构更加简单明了，也具有更好的可解释性。

### Transformer的缺陷及解决方案

#### 无位置信息

RNN的模型结构让它本身天然具备位置信息，但Transformer的self-attention结构是对序列进行并行处理，这就丢失了位置信息。

解决方案是在input中引入了位置编码，与word embedding相加后传入模型，让模型能够学习到位置信息。

### Transformer结构和细节

#### 结构

<figure><img src="https://pic3.zhimg.com/80/v2-e7319f29c51175fdc5256bb79441f27a_720w.webp" alt="" height="765" width="539"><figcaption></figcaption></figure>

1. 输入：word\_embedding + positional\_encoding
2. 编码器：编码器由6个相同的层组成，每层包含两个子层和残差+归一化结构
3.
   1. 多头自注意力层（multi-head self-attention）
   2. 前馈神经网络（feed forward）
4. 解码器：解码器也是由6个相同的层组成，每层包含三个子层和残差+归一化结构
5.
   1. 掩码多头自注意力层（masked multi-head self-attention）
   2. 多头自注意力层（multi-head self-attention），这一层是使用了编码器堆叠的输出
   3. 前馈神经网络（feed forward）
6. linear+softmax：通过linear转换维度，再通过softmax输出概率。
