# 2.2.3 RoPE

在基于Transformer的语言模型中，通常通过自注意力机制来利用各个token的位置信息，RoPE能够做到Q和K的内积的输入变量只有位置m和n的word embedding，以及相对位置m-n，也就是说，Q和K的内积应该只以相对位置的形式编码位置信息。

让我们用一个公式来表示RoPE：

$$
\langle f_q(\boldsymbol{x}_m,m),f_k(\boldsymbol{x}_n,n)\rangle=g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)
$$











1. **提出了RoPE方法**：作者们首先探讨了现有的相对位置编码方法，这些方法大多是基于将位置编码添加到上下文表示中的思想构建的。然后，他们引入了一种新颖的方法，即RoPE，利用旋转矩阵来编码相对位置，并且具有明确的理论解释。
2. **研究了RoPE的性质**：作者们研究了RoPE的性质，展示了它随着相对距离的增加而衰减的特性，这符合自然语言编码的期望。他们认为以前的相对位置编码方法与线性自注意力不兼容。
3. **在长文本基准数据集上评估了RoFormer**：作者们在多个长文本分类基准数据集上评估了采用RoPE增强的RoFormer模型。实验结果表明，RoFormer与基线模型相比，能够一致地取得更好的性能。



