# 4.3 attention-mask

#### attention mask

这时候有同学又要问了，既然attention机制可以同时学习到整个序列的信息，那么在进行解码时，我们不可能知道在未来我们要生成的序列信息吧？也就是说，在解码器，又怎么可能使用attention机制呢？

实际上，本篇论文自然也考虑到了这一点，为了让模型在生成任务中，生成序列时依赖于已经生成的序列（自回归特性），Transformer采用了一种特殊的掩码（masking）技术。

听起来复杂，实际上很简单，就是将softmax输入中未来位置的输出的值设置为一个非常大的负数，这样这些位置的权重就会接近于零，从而不会对当前位置的预测产生影响。
