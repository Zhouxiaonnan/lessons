# 5.1 Activation

Google的PaLM和Meta的LLaMA都使用了SwiGLU来增强Transformer架构中的FFN层的性能。SwiGLU是Gated Linear Units（GLU）激活函数的一种变体，由Noam Shazeer在论文《GLU Variants Improve Transformer》的论文中提出。本文主要介绍不同激活函数（如ReLU、GELU和Swish）在FFN层中的应用。

**使用ReLU激活的FFN**

Transformer模型通过多头注意力层和FFN层交替工作。FFN层存在于Transformer架构的编码器和解码器部分中。例如，下方的编码器块由多头注意力层和一个FFN层组成。

FFN层包括两个线性变换，中间插入一个非线性激活函数。最初的Transformer架构采用了ReLU激活函数。

其中ReLU的定义为`$\\text{ReLU}(x) = \\max(0, x)$`

在他们的实验中使用了不包含bias项的FFN，T5其实也是这么搞的

`$\\text{FFN}\_{\\text{ReLU}}(x, W\_1, W\_2) = \\text{ReLU}(xW\_1) W\_2$`

**使用GELU激活的FFN**

论文《Gaussian Error Linear Units（GELUs）》提出了GELU，这是ReLU的平滑版本。

![](https://ones.cn/wiki/api/wiki/editor/M5PiCb9N/GBgY3ds8/resources/ayJ\_98Hb\_jQNLfaiJx3za1gL1rUz17j3g-2p0Uew8DI.png)

这是一个处处可微的非线性函数，因此使用GELU的FFN层可以表示为

`$\\text{FFN}\_{\\text{GELU}}(x, W\_1, W\_2) = \\text{GELU}(xW\_1) W\_2 \\\\$`在GELU论文中，作者使用了标准正态分布的累积分布函数（cdf）的近似计算来提高计算速度。

```
from scipy.stats import norm

def gelu(x):
   return x * norm.cdf(x)
```

**ReLU 和 GeLU 的区别在于形状和计算效率**。ReLU 是一个非常简单的函数，仅仅是输入为负数时返回0，而输入为正数时返回自身，从而仅包含了一次分段线性变换。但是，**ReLU 函数存在一个问题，就是在输入为负数时，输出恒为0，这个问题可能会导致神经元死亡，从而降低模型的表达能力**。GeLU 函数则是一个连续的 S 形曲线，介于 Sigmoid 和 ReLU 之间，形状比 ReLU 更为平滑，可以在一定程度上缓解神经元死亡的问题。不过，由于 GeLU 函数中包含了指数运算等复杂计算，所以在实际应用中通常比 ReLU 慢。

**使用Swish激活的FFN**

![](https://ones.cn/wiki/api/wiki/editor/M5PiCb9N/GBgY3ds8/resources/rnlFqqW1IHlqyK\_SlrKdWbrae8hVHCDWF2K8\_iyGsv0.png)

\


论文《Swish: a Self-Gated Activation Function》提出了Swish，这也是对带有非零负值梯度的ReLU平滑版本。

```
import numpy as np

def swish(x, beta=1):
   return x * (1 / (1 + np.exp(-beta * x)))
```

Swish同样是个处处可微的非线性函数，且有一个参数beta用于控制函数的形状，带有Swish的FFN层可以表示为

`$\\text{FFN}\_{\\text{Swish}}(x, W\_1, W\_2) = \\text{Swish}_1(xW\_1) W\_2 \\\\$`其中`$\\text{Swish}_{\\beta}(x) = x \\sigma(\\beta x)$`，在他们的实验中，beta取为1。

Swish函数的特点是在接近零的区域表现得类似于线性函数，而在远离零的区域则表现出非线性的特性。相比于其他常用的激活函数（如ReLU、tanh等），Swish函数在某些情况下能够提供更好的性能和更快的收敛速度。

Swish函数的设计灵感来自于自动搜索算法，它通过引入一个可调节的超参数来增加非线性程度。当beta为0时，Swish函数退化为线性函数；当beta趋近于无穷大时，Swish函数趋近于ReLU函数。

需要注意的是，Swish函数相对于其他激活函数来说计算开销较大，因为它需要进行Sigmoid运算。因此，在实际应用中，也可以根据具体情况选择其他的激活函数来代替Swish函数。

**GLU及其变体**

GLU（Gated Linear Units）其实不算是一种激活函数，而是一种神经网络层。它是一个线性变换后面接门控机制的结构。其中门控机制是一个sigmoid函数用来控制信息能够通过多少。

`$\\text{GLU}(x, W, V, b, c) = \\sigma(xW + b) \\otimes (xV + c) \\\\$`其中 `$\\sigma$` 为sigmoid函数，`$\\otimes$` 为逐元素乘。通过使用其他的激活函数我们就能够得到GLU的各种变体了。

比如说现在LLM中常用的SwiGLU其实就是**采用Swish作为激活函数的GLU**变体

`$\\text{SwiGLU}(x,W, V, b, c) = \\text{Swish}\_1(xW + b) \\otimes (xV + c) \\\\$`由于引入了更多的权重矩阵，通常会**对隐藏层的大小做一个缩放**，从而保证整体的参数量不变。

代码的实现如下

```
class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):
        super().__init__()
        hidden_dim = multiple_of * ((2 * hidden_dim // 3 + multiple_of - 1) // multiple_of)
        self.w1 = nn.Linear(dim, hidden_dim)
        self.w2 = nn.Linear(hidden_dim, dim)
        self.w3 = nn.Linear(dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))
```

为什么这里使用的是silu激活函数？ 因为SiLU其实就是beta为1时的Swish激活函数

`$f(x) = x \\cdot \\sigma(x) \\\\$`GLU线性门控单元的特点是能够对输入向量进行选择性地激活，从而增强模型的表达能力。它在Transformer模型的编码器和解码器中广泛应用，用于对输入向量进行非线性变换和特征提取。

需要注意的是，GLU线性门控单元的计算复杂度较高，可能会增加模型的计算开销。因此，在实际应用中，也可以根据具体情况选择其他的非线性变换方式来代替GLU线性门控单元。
