# 3.3 从Post-Norm到Pre-Norm

PostLN

Post-LN 是在 Transformer 的原始版本中使用的归一化方案。在此方案中，每个子层（例如，自注意力机制或前馈网络）的输出先通过子层自身的操作，然后再通过层归一化（Layer Normalization）。

```
SubLayerOutput = SubLayer(Input)
Output = LayerNorm(SubLayerOutput + Input)
```

但是后来在LLM中训练过程中发现，PostLN的输出层附近的梯度过大会造成训练的不稳定性，因此现在的LLM很少单独使用PostLN

<figure><img src="../.gitbook/assets/image (3).png" alt=""><figcaption></figcaption></figure>

PreLN

与 Post-LN 相反，Pre-LN 是先对输入进行层归一化，然后再传递到子层操作中。这样的顺序对于训练更深的网络可能更稳定，因为归一化的输入可以帮助缓解训练过程中的梯度消失和梯度爆炸问题。Pre-LN 的每一层结构如下：

```
NormalizedInput = LayerNorm(Input)
SubLayerOutput = SubLayer(NormalizedInput)
Output = SubLayerOutput + Input
```

PostLN和PreLN对比：

<figure><img src="../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>
