# 2.2.1 绝对位置编码

### 为什么需要位置编码

在语言模型的发展历程中，我们逐渐发现模型对于语义的理解与文字在语句中的位置是有很大关系的，比如“狗咬了我”和“我咬了狗”，这两句句子的语义是完全不同的，造成语义不同的原因就在于“我”和“狗”的位置发生了互换，而如果在语言模型中，我们只是对字词进行编码，而忽略了句子中各个字词的位置，那么模型很可能将以上两句话理解为相同的意思。

### 可训练的绝对位置编码

于是我们自然可以想到，想要模型能够学习到位置信息带来的语义变化，那我们就每个子词都赋予一个位置就行了，所以就诞生了绝对位置编码。这里先解释一个概念，什么叫做绝对位置，其实就是比如一段话“我咬了狗”，假设现在我们的分词器就是按照词语进行分词，那么我们可以将这句话切分为“我 咬了 狗”，其中“我”的绝对位置是0，“咬了”的绝对位置是1，“狗”的绝对位置是2。

| 子词 | 位置 |
| -- | -- |
| 我  | 0  |
| 咬了 | 1  |
| 狗  | 2  |

简单来说，绝对位置就是字词在这个完整的句子中，从开头开始数的位置。诶？然后我们发现，这个位置编码和子词编码有点像哦？既然我们可以初始化子词编码的embedding，然后在训练中获得每个子词的向量，那么我们是不是也可以通过初始化绝对位置的embedding，也在训练中获得每个绝对位置的向量呢？当然可以了，这就是可以训练的绝对位置编码。那么在这种方法中，当我们对模型训练完毕，不仅可以得到一个子词对应向量的mapping，也可以得到一个绝对位置对应向量的mapping。当然绝对位置编码与词向量之间可以用多种方式进行融合，比如直接相加，相乘，或者拼接到一起都是可以的。

#### 𝑖𝑛𝑝𝑢𝑡\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔=𝑤𝑜𝑟𝑑\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔+𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑖𝑛𝑝𝑢𝑡\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔=𝑤𝑜𝑟𝑑\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔∗𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑖𝑛𝑝𝑢𝑡\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔=𝑐𝑜𝑛𝑐𝑎𝑡\[𝑤𝑜𝑟𝑑\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔,𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛\_𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔]缺点

现在我们的模型都在要求越来越大的输入长度，而这种绝对位置编码就没有办法处理这个问题，也就是外推性很差。原因也很简单，我们在训练时一定会存在一个最大训练长度，那么每一个训练句子的长度就被固定死了，所以编码的最大位置也被固定死了。此时如果我们输入的长度是大于位置编码最大长度，那么超出的部分的位置是没有进行位置编码训练的，它找不到一个对应的位置向量，这就会降低模型的效果。

### Sinusoidal绝对位置编码

相信只要了解一些LLM技术的同学一定听说过Sinusoidal位置编码，这种编码的第一次提出来自于有名的论文《attention is all you need》

<figure><img src="https://pic1.zhimg.com/80/v2-048e939dc8f869f21f1a6dcdacb10ae6_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

添加图片注释，不超过 140 字（可选）

在这篇论文中对于位置编码采用的英文是"positional encoding"而非"position embedding"，两者差别如下：

* embedding一般是指学习出来的，“嵌入”到空间中的编码
* encoding是直接通过一个函数计算出来的

那么sinusoidal位置编码到底是如何计算的？直接看看论文中是怎么说的吧：

<figure><img src="https://picx.zhimg.com/80/v2-0ab0f7083b1a38973794de29b132af9d_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

添加图片注释，不超过 140 字（可选）

其中：

* pos：位置
* i：embedding不同位置的索引
* d：embedding的维度

举一个例子，如果embedding维度位128维，对于第二个token，它的位置编码如下：𝑃𝐸(2,0)=𝑠𝑖𝑛(2/100000/128)𝑃𝐸(2,1)=𝑐𝑜𝑠(2/100001/128)𝑃𝐸(2,2)=𝑠𝑖𝑛(2/100002/128)𝑃𝐸(2,3)=𝑐𝑜𝑠(2/100003/128)

#### 能够学习到相对位置？

关于这种编码方式，论文中还有一段十分令人困惑的话：

> We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).

啥意思呢，就是作者说，我们选择这种位置编码方式，就是因为我们“觉得”它能够让模型很简单地学习到tokens的相对位置，毕竟对于任何相对距离k，PE(pos+k)可以被PE(pos)线性表示。emmm...竟然是“觉得”？论文中并没有任何严格的数学证明，而到现在，其实对于为什么sinusoidal可以学到相对位置信息仍然没有一个定论（也有可能我孤陋寡闻！如果有证明，请在评论区答疑解惑！），不过猜测倒是有很多，比如[Transformer升级之路：1、Sinusoidal位置编码追根溯源 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/359500899) 在这篇文章中，作者苏剑林对Sinosoidal编码方式进行了更加细致的推导和说明，从数学上给出了一个为什么这种编码方式可以学习到相对位置的可能。我们这里就简单地给出一个证明方式：首先我们知道在attention中计算QK的点积公式：𝐴𝑖,𝑗𝑎𝑏𝑠=(𝑊𝑞𝑄)𝑇(𝑊𝑘𝐾)𝐴𝑖,𝑗𝑎𝑏𝑠=(𝑊𝑞(𝐸𝑥𝑖+𝑈𝑖))𝑇(𝑊𝑘(𝐸𝑥𝑗+𝑈𝑗))其中，xi和xj代表的是在位置i和位置j的token，E代表token的embedding，U代表位置的encoding，这里E+U，就是在输入时对两个向量进行相加的操作。现在我们将其进行展开：𝐴𝑖,𝑗𝑎𝑏𝑠=𝐸𝑥𝑖⊤𝑊𝑞⊤𝑊𝑘𝐸𝑥𝑗⏟(𝑎)+𝐸𝑥𝑖⊤𝑊𝑞⊤𝑊𝑘𝑈𝑗⏟(𝑏)+𝑈𝑖⊤𝑊𝑞⊤𝑊𝑘𝐸𝑥𝑗⏟(𝑐)+𝑈𝑖⊤𝑊𝑞⊤𝑊𝑘𝑈𝑗⏟(𝑑)可以看到除了(d)项之外，其他三项要么是没有位置信息，要么是只有一个位置信息，只有(d)包含两个位置信息，但现在这依然是绝对位置，而我们之前说过能够学习到相对位置，也就是(j-i)=k的信息，所以我们将(d)项提出来，忽略掉两个可以学习的权重矩阵，再进行推导：首先根据sinusoidal公式，将位置i的编码进行展开，可以得到：𝑈𝑖=\[sin⁡(𝑐0𝑖)cos⁡(𝑐0𝑖)⋮sin⁡(𝑐𝑑2−1𝑖)cos⁡(𝑐𝑑2−1𝑖)]其中由于此时维度d已经固定，所以我们就用c来表示三角函数中位置i的除项，然后我们计算两个位置编码的点积：𝑈𝑖𝑇𝑈𝑖+𝑘=∑𝑗=0𝑑2−1\[sin⁡(𝑐𝑗𝑖)sin⁡(𝑐𝑗(𝑖+𝑘))+cos⁡(𝑐𝑗𝑖)cos⁡(𝑐𝑗(𝑖+𝑘))]=∑𝑗=0𝑑2−1cos⁡(𝑐𝑗(𝑖−(𝑖+𝑘)))=∑𝑗=0𝑑2−1cos⁡(𝑐𝑗𝑘)于是我们愉快地发现，原来两个位置的点积只与它们的相对位置有关，这样模型就学到了相对位置信息。那么它还有什么特性呢？

#### 远程衰减性

远程衰减，意思就是随着两个tokens相对距离的增加，两者的内积会变得越来越小。这其实很符合我们的直觉，因为一般来说，一句话中距离越近的两个tokens，它们之间的联系越紧密，两个隔得老远的tokens，可能根本没有语义上的联系。

<figure><img src="https://pic1.zhimg.com/80/v2-c7e0efc0cfd31e0d9df7f53694c5bda7_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

远程衰减性，k为相对距离

但是！还记得在推导怎么学习相对位置信息的(d)项中，我们忽略了两个权重矩阵，而这两个矩阵在最开始是要进行随机初始化的！那么当我么再将这个权重矩阵乘上之后，会发生什么呢？没错，远程衰减性消失了！这意味着，实际上在attention中，并没有真正学习到相对位置信息。

<figure><img src="https://picx.zhimg.com/80/v2-4e4453cf91245e55c44728060e8c246c_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

添加图片注释，不超过 140 字（可选）

#### base为什么是10000？

显然在编码公式中这个常数“10000”是一个通过实验而得到的效果较好的经验值，不过我们也可以通过一些方式分析一下为什么？我们都知道sin和cos是周期性函数，当base比较小的时候，在同一个维度i上，随着位置pos的增加，PE会呈周期性变化，如果周期较小，那么我们可能经过几个或者几十个pos之后，就会得到一个相同的值，也就是说在一个较小的相对位置k上，两个tokens的位置编码是一致的，那么我们就没有办法很好地区分这两个tokens的位置。所以为了在至少在相对位置k较大的一个区间内，让模型可以区分tokens的位置，就选择了10000（或者当然也可以是别的比较大的值）作为base的值。

#### 不同维度上的周期性

通过下图我们可以看到sinosoidal编码再不同的维度下，它的周期性也是不同的。

<figure><img src="https://picx.zhimg.com/80/v2-47cc3463c42371c23ee99e05c974c2fd_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

添加图片注释，不超过 140 字（可选）

这种特性能够让模型在不同的维度上学习到不同频率的位置信息，这也防止了位置编码在每个维度上的值相同的情况。一个简单的理解就是，我们在上图dim4的波峰分别画两条竖线，那么这两个tokens在dim4上的位置编码是一致的，但是在dim5，dim6，dim7上的位置编码都不一致，并且两者的差值也不同。

#### 优势

* 能够表示相对位置信息
* 使用sin和cos的形式可以让位置变化的信号更加平滑
* 远程衰减性能够让模型更专注于相对距离较近的tokens语义关联性
* 相对于可学习的绝对位置编码，Sinosoidal因为能够学习到相对位置信息，它的外推性更好

#### 缺点

* 目前还没有一个合理的解释为什么能够获得相对位置信息
* 从数学上来看，并非严格验证的方式，而更像是一种经验公式
* 没有真正学习到相对位置信息

###
