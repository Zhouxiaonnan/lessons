# 3.1.1 Normalization

layerNorm

<figure><img src="../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/image (7).png" alt=""><figcaption></figcaption></figure>

下面的代码定义了一个适用于 Transformer 中的 Layer Normalization 层。

```
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    def __init__(self, num_features, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True, unbiased=False)
        normalized_x = (x - mean) / (std + self.eps)
        return self.gamma * normalized_x + self.beta
```

DeepNorm

DeepNorm是由微软提出的一种Normalization方法，主要对Transformer结构中的残差链接做修正，是一种PostLN结构

DeepNorm可以缓解模型参数爆炸式更新的问题，把模型参数更新限制在一个常数域范围内，使得模型训练过程可以更稳定。模型规模可以达到1000层。

DeepNorm结合了Post-LN的良好性能以及Pre-LN的训练稳定性。与Post-LN 相比，DeepNorm在执行层归一化之前Up-Scale了残差连接。

具体的实现，可以参照下图，DeepNorm对layerNorm之前的残差链接进行了up-scale（ 𝛼>1 ），在初始化阶段down-scale了模型参数（ 𝛽<1 ）。

<figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>
