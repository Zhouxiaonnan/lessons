# 1.1 语料标准化

做LLM会需要大量的语料进行训练，其中的很大一部分都是从网络上得到，而语料的质量又决定了模型的能力，因此数据清洗就有着大量的工作，其中一项，就是要对数据进行标准化操作，也就是Normalization。

在语料标准化上，常用的就是四个概念，NFD（Normalization Form Decomposition），NFC（Normalization Form Composition），NFKD（Normalization Form Compatibility Decomposition），NFKC（Normalization Form Compatibility Composition）

其中：

* Decomposition：分解
* Composition：组合
* Compatibility：兼容相等
* 没有Compatibility：规范相等

分解和组合很好理解，就是对字符进行分解和组合，比如对于一个字符'≠'，但经过NFD拆解后，得到的看似是'≠'的字符已经变成了由两个unicode编码组成了。

```
>>> hex(ord('#'))
'0x2260'
>>> hex(ord('#'))
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: ord() expected a character, but stringg of length 2 found
>>> hex(ord('#'[0]))
0x3d"
>>> hex(ord('#'[1]))
0x338'
```

规范相等：可以简单理解为要求一模一样的相等，比如对于'≠'，拆解后与原来的字符对比，打印出来的外观是相同的，含义也是相同的。

兼容相等：相比于规范相等，兼容相等的要求没有那么严格，这种相等是比较含糊的，比如不同字体的文字，经过兼容相等的拆解后，得到统一的字体文字，但打印出来或许看起来就不一致了。比如：

```
NFD
Abc R
Abc K
盘
蟹
5
5
***********************大大大大
NFKD
'Abc 贝
凝'5
Abc火株式会社'8

```

在NFD下，由于要求规范相等，因此NFD并没有对字符进行拆解，而在NFKD下，只要求兼容相等，所以对字符进行了拆解。

组合也是一样的，NFC和NFKC会对输入的字符进行拆解再进行组合，比如：

```
NFC
≠é蟹" 3
≠é凝 '
3
******************************
NFKC
≠é蟹' 3
#é株式会社  6
***********************************************************

```

在以上的例子中，NFC要求规范相等，所以没有对“株式会社”进行拆解，但实际上是对前两个字符进行了拆解，最终再进行规范等价的组合。

而NFKC因为只需要兼容相等，所以也会对“株式会社”进行兼容相等的拆解。

这对于LLM来说的意义在哪里呢？

* 减少词表大小
* 减少训练量
* 减少推理时用户输入造成的歧义（因为输入后也会对文本进行规范化）

在python中可以直接通过导入tokenizers.normalizers包来进行以上四种标准化。
