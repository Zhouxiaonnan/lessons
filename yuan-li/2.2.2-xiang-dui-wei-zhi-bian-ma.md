# 2.2.2 相对位置编码

### RPE(Relative Position Encoding)

论文：[\[1803.02155\] Self-Attention with Relative Position Representations (arxiv.org)](https://arxiv.org/abs/1803.02155)为了解决sinusoidal编码在attention中没有学习到相对位置信息的问题，RPE登场了，这是一种可学习的相对位置编码方式。

#### RPE的实现

RPE的思路其实很简单，既然attention中没有相对位置信息，那我手动加上一些可训练权重，让这些权重代表相对位置信息不就好了嘛。首先我们先把self-attention的公式进行一下阶段分解：𝑧𝑖=∑𝑗=1𝑛𝛼𝑖𝑗(𝑥𝑗𝑊𝑉)𝛼𝑖𝑗=exp⁡𝑒𝑖𝑗∑𝑘=1𝑛exp⁡𝑒𝑖𝑘𝑒𝑖𝑗=(𝑥𝑖𝑊𝑄)(𝑥𝑗𝑊𝐾)𝑇𝑑𝑧接下来，我们就要往以上的公式中添加相对位置信息了！首先论中是将self-attention模型看成一个全连接有向图，那么每两个位置之间其实都有一个边，比如对于位置 𝑥𝑖,𝑥𝑗 ，连接两者的边可以表示为两个向量 𝑎𝑖𝑗𝑉,𝑎𝑖𝑗𝐾 ，而这两个就是模型自行学习的相对位置信息。为了让模型能够学到这两个向量，我们将以上self-attention的三个公式修改为：𝑧𝑖=∑𝑗=1𝑛𝛼𝑖𝑗(𝑥𝑗𝑊𝑉+𝑎𝑖𝑗𝑉)𝛼𝑖𝑗=exp⁡𝑒𝑖𝑗∑𝑘=1𝑛exp⁡𝑒𝑖𝑘𝑒𝑖𝑗=(𝑥𝑖𝑊𝑄)(𝑥𝑗𝑊𝐾+𝑎𝑖𝑗𝐾)𝑇𝑑𝑧比如对于第1个和第2个位置，它们之间的边就可以表示为：

<figure><img src="https://pic1.zhimg.com/80/v2-cd53ca93a8c898118c56bf19058e39f2_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

添加图片注释，不超过 140 字（可选）

#### 优化计算效率

虽然仅仅是在self-attention公式中添加了两个可学习的向量，但在实际中，这计算量增加的不是一点两点，所以为了减少计算，还对其做了一定的优化。首先，RPE假定当相对位置距离超过某一个值k时，相对位置信息就不再有用，因此设计了一个clip函数，对于一个位于位置i的token而言，只考虑\[i-k, i+k]的相对位置信息。𝑎𝑖𝑗𝐾=𝑤clip(𝑗−𝑖,𝑘)𝐾𝑎𝑖𝑗𝑉=𝑤clip(𝑗−𝑖,𝑘)𝑉clip⁡(𝑥,𝑘)=max(−𝑘,min(𝑘,𝑥))由此可得：𝑤𝐾=(𝑤−𝑘𝐾,…,𝑤𝑘𝐾)𝑤𝑉=(𝑤−𝑘𝑉,…,𝑤𝑘𝑉)在测试中，发现k超过4效果就没有提升了，所以RPE默认是k=4为了优化空间复杂度，相对位置表示是在各个头中共享的。

#### 测试

在翻译任务中，还分别对两个新加入的相对位置表示的影响进行了测试，测试结果表示𝑎𝑖𝑗𝑉对翻译任务没有影响，而对于其他任务而言，在论文中也说需要进一步进行验证。

<figure><img src="https://picx.zhimg.com/80/v2-7865a82ea85e2b6ac81badbf07cfcf5a_720w.png?source=d16d100b" alt=""><figcaption></figcaption></figure>

添加图片注释，不超过 140 字（可选）

### RPE之二（Transformer-XL RPE）

除了上述的相对位置编码之外，在另一篇论文：[\[1901.02860\] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (arxiv.org)](https://arxiv.org/abs/1901.02860)中，也提出了另外一种可学习的相对位置编码方式。还是先来看一下self-attention展开后的几项：𝐴𝑖,𝑗𝑎𝑏𝑠=𝐸𝑥𝑖⊤𝑊𝑞⊤𝑊𝑘𝐸𝑥𝑗⏟(𝑎)+𝐸𝑥𝑖⊤𝑊𝑞⊤𝑊𝑘𝑈𝑗⏟(𝑏)+𝑈𝑖⊤𝑊𝑞⊤𝑊𝑘𝐸𝑥𝑗⏟(𝑐)+𝑈𝑖⊤𝑊𝑞⊤𝑊𝑘𝑈𝑗⏟(𝑑)那么这里提出的RPE并不是直接添加两个代表相对位置信息的可学习参数，而是对展开式中的各项进行修改。𝐴𝑖,𝑗rel=𝐸𝑥𝑖⊤𝑊𝑞⊤𝑊𝑘,𝐸𝐸𝑥𝑗⏟(𝑎)+𝐸𝑥𝑖⊤𝑊𝑞⊤𝑊𝑘,𝑅𝑅𝑖−𝑗⏟(𝑏)+𝑢⊤𝑊𝑘,𝐸𝐸𝑥𝑗⏟(𝑐)+𝑣⊤𝑊𝑘,𝑅𝑅𝑖−𝑗⏟(𝑑).首先，将(b)和(d)项中的绝对位置编码 𝑈𝑗 替换成了相对位置编码 𝑅𝑖−𝑗 ，这里的 𝑅𝑖−𝑗 也是采用了Sinusoidal编码方式，因此这不是一个可学习的编码。第二，将 𝑈𝑖⊤𝑊𝑞⊤ 分别转换为 𝑢⊤ 和 𝑣⊤ ，这是两个可以学习的参数向量。为什么这样做呢？因为在相对编码中，我们不需要考虑绝对位置，也就是 𝑈𝑖 的信息，所以转换为可学习向量。第三，将 𝑊𝑘 拆分成两个部分，分别适配world embedding（ 𝑊𝑘,𝐸 ）和relative positional encoding（ 𝑊𝑘,𝑅 ），也就是说在这种attention中，内容和位置的信息是分开进行学习的。
